*** Begin Patch
*** Add File: services/api_frontend/app/webhooks.py
+import os
+import hmac
+import hashlib
+import uuid
+import time
+from fastapi import APIRouter, Request, HTTPException, BackgroundTasks
+from .db import messages_collection
+from .kafka_producer import kafka_producer
+
+router = APIRouter()
+
+META_APP_SECRET = os.getenv("META_APP_SECRET", "")
+TELEGRAM_SECRET_TOKEN = os.getenv("TELEGRAM_SECRET_TOKEN", "")
+
+def verify_meta_signature(body: bytes, signature: str) -> bool:
+    if not signature:
+        return False
+    mac = hmac.new(META_APP_SECRET.encode(), msg=body, digestmod=hashlib.sha256)
+    expected = "sha256=" + mac.hexdigest()
+    return hmac.compare_digest(expected, signature)
+
+@router.post("/webhooks/meta")
+async def meta_webhook(request: Request, background_tasks: BackgroundTasks):
+    """
+    Generic endpoint to receive Meta / WhatsApp / Instagram notifications.
+    Verifies X-Hub-Signature-256 if META_APP_SECRET is present.
+    Quickly acknowledges and publishes raw event to incoming.webhooks topic for async processing.
+    """
+    body = await request.body()
+    sig = request.headers.get("X-Hub-Signature-256")
+    if META_APP_SECRET:
+        if not verify_meta_signature(body, sig):
+            raise HTTPException(status_code=401, detail="invalid signature")
+
+    event_id = str(uuid.uuid4())
+    raw = body.decode("utf-8", errors="ignore")
+    # Publish to Kafka for async normalization/processing
+    await kafka_producer.send("incoming.webhooks", event_id, {"raw": raw, "source": "meta", "received_at": time.time()})
+    # persist immutable raw for audit
+    await messages_collection.insert_one({"_id": event_id, "source": "meta", "raw": raw, "ingested_at": time.time(), "processed": False})
+    return {"status": "accepted", "id": event_id}
+
+@router.post("/webhooks/telegram")
+async def telegram_webhook(request: Request, background_tasks: BackgroundTasks):
+    """
+    Telegram webhook endpoint. If TELEGRAM_SECRET_TOKEN is configured, verifies
+    X-Telegram-Bot-Api-Secret-Token header.
+    """
+    secret = request.headers.get("X-Telegram-Bot-Api-Secret-Token")
+    if TELEGRAM_SECRET_TOKEN and secret != TELEGRAM_SECRET_TOKEN:
+        raise HTTPException(status_code=401, detail="invalid webhook secret")
+    body = await request.body()
+    event_id = str(uuid.uuid4())
+    raw = body.decode("utf-8", errors="ignore")
+    await kafka_producer.send("incoming.webhooks", event_id, {"raw": raw, "source": "telegram", "received_at": time.time()})
+    await messages_collection.insert_one({"_id": event_id, "source": "telegram", "raw": raw, "ingested_at": time.time(), "processed": False})
+    return {"status": "accepted", "id": event_id}
+
*** End Patch
*** Begin Patch
*** Add File: services/worker/app/normalizers.py
+"""
+normalizers.py
+Converts provider webhook JSONs into internal message model used by the system.
+Each function returns either None (if not convertible) or a dict with the internal
+message schema:
+{
+  "message_id": str,
+  "conversation_id": str,
+  "sender_id": str,
+  "recipient_ids": [...],
+  "payload_type": "text"|"image"|...,
+  "payload_ref": str | {"url": "...", "mime":"...", "caption": "..."},
+  "metadata": {...},
+  "created_at": float(timestamp)
+}
+"""
+import time
+import uuid
+from typing import Optional, Dict, Any
+
+def normalize_meta_event(raw_json: Dict[str, Any]) -> Optional[Dict[str, Any]]:
+    # Simplified normalization for WhatsApp Cloud / Meta notifications.
+    # The real payload is nested; this extracts the most common text message form.
+    try:
+        entries = raw_json.get("entry", [])
+        for e in entries:
+            changes = e.get("changes", [])
+            for ch in changes:
+                value = ch.get("value", {})
+                messages = value.get("messages")
+                if messages:
+                    for m in messages:
+                        mid = m.get("id") or str(uuid.uuid4())
+                        from_id = m.get("from")
+                        text = None
+                        if "text" in m:
+                            text = m["text"].get("body")
+                            return {
+                                "message_id": mid,
+                                "conversation_id": f"wa:{from_id}",
+                                "sender_id": f"wa:{from_id}",
+                                "recipient_ids": [],  # worker will resolve recipients
+                                "payload_type": "text",
+                                "payload_ref": text,
+                                "metadata": {"provider": "whatsapp"},
+                                "created_at": time.time()
+                            }
+                        # media handling
+                        if m.get("type") in ("image", "document", "video"):
+                            media = m.get(m.get("type"), {})
+                            return {
+                                "message_id": mid,
+                                "conversation_id": f"wa:{from_id}",
+                                "sender_id": f"wa:{from_id}",
+                                "recipient_ids": [],
+                                "payload_type": m.get("type"),
+                                "payload_ref": {"url": media.get("url"), "mime": media.get("mime_type"), "caption": media.get("caption")},
+                                "metadata": {"provider": "whatsapp"},
+                                "created_at": time.time()
+                            }
+        return None
+    except Exception:
+        return None
+
+def normalize_telegram_event(raw_json: Dict[str, Any]) -> Optional[Dict[str, Any]]:
+    try:
+        # Telegram webhook payload has 'message' or 'edited_message'
+        msg = raw_json.get("message") or raw_json.get("edited_message")
+        if not msg:
+            return None
+        mid = str(msg.get("message_id", "") )
+        from_user = msg.get("from", {})
+        chat = msg.get("chat", {})
+        text = msg.get("text")
+        if text:
+            return {
+                "message_id": f"tg:{mid}",
+                "conversation_id": f"tg:{chat.get('id')}",
+                "sender_id": f"tg:{from_user.get('id')}",
+                "recipient_ids": [],
+                "payload_type": "text",
+                "payload_ref": text,
+                "metadata": {"provider": "telegram"},
+                "created_at": time.time()
+            }
+        # photo/document handling
+        if msg.get("photo") or msg.get("document"):
+            # choose the largest photo
+            if msg.get("photo"):
+                sizes = msg["photo"]
+                file_id = sizes[-1]["file_id"]
+                return {
+                    "message_id": f"tg:{mid}",
+                    "conversation_id": f"tg:{chat.get('id')}",
+                    "sender_id": f"tg:{from_user.get('id')}",
+                    "recipient_ids": [],
+                    "payload_type": "image",
+                    "payload_ref": {"file_id": file_id},
+                    "metadata": {"provider": "telegram"},
+                    "created_at": time.time()
+                }
+            if msg.get("document"):
+                doc = msg["document"]
+                return {
+                    "message_id": f"tg:{mid}",
+                    "conversation_id": f"tg:{chat.get('id')}",
+                    "sender_id": f"tg:{from_user.get('id')}",
+                    "recipient_ids": [],
+                    "payload_type": "document",
+                    "payload_ref": {"file_id": doc.get("file_id"), "mime": doc.get("mime_type")},
+                    "metadata": {"provider": "telegram"},
+                    "created_at": time.time()
+                }
+        return None
+    except Exception:
+        return None
+
+def normalize_provider_event(raw_payload: Dict[str, Any]) -> Optional[Dict[str, Any]]:
+    src = raw_payload.get("source")
+    raw = raw_payload.get("raw")
+    if not raw:
+        return None
+    # raw may be stringified JSON
+    import json
+    try:
+        parsed = json.loads(raw)
+    except Exception:
+        try:
+            parsed = raw_payload.get("body") or {}
+        except Exception:
+            return None
+    if src == "meta":
+        return normalize_meta_event(parsed)
+    if src == "telegram":
+        return normalize_telegram_event(parsed)
+    # extend for instagram etc.
+    return None
+
*** End Patch
*** Begin Patch
*** Add File: services/api_frontend/app/uploads.py
+"""
+uploads.py
+Multipart upload orchestration endpoints:
+- init_multipart: create multipart in S3/MinIO
+- presign_part: generate presigned url for a part
+- complete: finalize multipart and optionally trigger provider upload
+"""
+import os
+import time
+import uuid
+import boto3
+import aiohttp
+from botocore.client import Config
+from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
+from .db import file_uploads_collection
+from .auth import get_current_user
+
+router = APIRouter()
+
+S3_ENDPOINT = os.getenv("MINIO_ENDPOINT", "http://minio:9000")
+S3_ACCESS = os.getenv("MINIO_ACCESS_KEY", "minioadmin")
+S3_SECRET = os.getenv("MINIO_SECRET_KEY", "minioadmin")
+S3_BUCKET = os.getenv("MINIO_BUCKET", "chat4all")
+PHONE_ID = os.getenv("WHATSAPP_PHONE_ID", "")
+WA_TOKEN = os.getenv("WHATSAPP_TOKEN", "")
+
+_s3 = boto3.client(
+    "s3",
+    endpoint_url=S3_ENDPOINT,
+    aws_access_key_id=S3_ACCESS,
+    aws_secret_access_key=S3_SECRET,
+    config=Config(signature_version="s3v4"),
+    region_name="us-east-1",
+)
+
+@router.post("/uploads/init_multipart")
+async def init_multipart(owner_id: str, filename: str, user: str = Depends(get_current_user)):
+    object_key = f"{owner_id}/{str(uuid.uuid4())}/{filename}"
+    resp = _s3.create_multipart_upload(Bucket=S3_BUCKET, Key=object_key)
+    upload_id = resp["UploadId"]
+    await file_uploads_collection.insert_one({
+        "upload_id": upload_id,
+        "object_key": object_key,
+        "bucket": S3_BUCKET,
+        "parts": [],
+        "status": "IN_PROGRESS",
+        "owner_id": owner_id,
+        "created_at": time.time()
+    })
+    return {"upload_id": upload_id, "object_key": object_key, "bucket": S3_BUCKET}
+
+@router.get("/uploads/{upload_id}/presign_part")
+async def presign_part(upload_id: str, part_number: int, user: str = Depends(get_current_user)):
+    doc = await file_uploads_collection.find_one({"upload_id": upload_id})
+    if not doc:
+        raise HTTPException(status_code=404, detail="upload not found")
+    url = _s3.generate_presigned_url('upload_part', Params={'Bucket': doc['bucket'], 'Key': doc['object_key'], 'UploadId': upload_id, 'PartNumber': part_number}, ExpiresIn=3600)
+    return {"url": url}
+
+@router.post("/uploads/{upload_id}/complete")
+async def complete_upload(upload_id: str, parts: list, background_tasks: BackgroundTasks, user: str = Depends(get_current_user)):
+    doc = await file_uploads_collection.find_one({"upload_id": upload_id})
+    if not doc:
+        raise HTTPException(status_code=404, detail="upload not found")
+    bucket = doc["bucket"]; key = doc["object_key"]
+    # parts must be list of {"PartNumber": n, "ETag": "..."}
+    try:
+        resp = _s3.complete_multipart_upload(Bucket=bucket, Key=key, UploadId=upload_id, MultipartUpload={"Parts": parts})
+    except Exception as e:
+        raise HTTPException(status_code=500, detail=f"complete multipart failed: {e}")
+    await file_uploads_collection.update_one({"upload_id": upload_id}, {"$set": {"status": "COMPLETED", "completed_at": time.time(), "s3_etag": resp.get("ETag")}})
+    # trigger provider upload in background
+    background_tasks.add_task(_upload_media_to_provider_stream, upload_id)
+    return {"ok": True, "s3_etag": resp.get("ETag")}
+
+async def _upload_media_to_provider_stream(upload_id: str):
+    """
+    Streams the object from S3/MinIO to provider (WhatsApp Cloud) to obtain provider_media_id
+    Avoids loading whole file into memory.
+    """
+    doc = await file_uploads_collection.find_one({"upload_id": upload_id})
+    if not doc:
+        return
+    bucket = doc["bucket"]; key = doc["object_key"]
+    # generate presigned GET so provider can fetch if supported
+    presigned_get = _s3.generate_presigned_url('get_object', Params={'Bucket': bucket, 'Key': key}, ExpiresIn=3600)
+    # For WhatsApp Cloud: prefer to upload via 'file' field using streaming
+    wa_upload_url = f"https://graph.facebook.com/v17.0/{PHONE_ID}/media"
+    params = {"access_token": WA_TOKEN}
+    try:
+        async with aiohttp.ClientSession() as session:
+            # attempt provider fetch via link first (some providers accept 'url' in payload)
+            payload = {"messaging_product": "whatsapp", "type": "image", "image": {"link": presigned_get}}
+            async with session.post(wa_upload_url, params=params, json=payload, timeout=60) as r:
+                text = await r.text()
+                if 200 <= r.status < 300:
+                    j = await r.json()
+                    media_id = j.get("id")
+                    await file_uploads_collection.update_one({"upload_id": upload_id}, {"$set": {"provider_media_id": media_id}})
+                    return media_id
+            # fallback: stream object and upload as file field (less ideal due to memory)
+            # using streaming with aiohttp: download in chunks and stream using multipart writer
+            get_url = _s3.generate_presigned_url('get_object', Params={'Bucket': bucket, 'Key': key}, ExpiresIn=3600)
+            async with session.get(get_url) as sresp:
+                if sresp.status != 200:
+                    await file_uploads_collection.update_one({"upload_id": upload_id}, {"$set": {"provider_upload_error": f"get_object {sresp.status}"}})
+                    return None
+                # stream to provider using data generator
+                data_gen = sresp.content.iter_chunked(65536)
+                mp = aiohttp.FormData()
+                # read into a temp file? for simplicity, buffer small objects
+                body = await sresp.read()
+                mp.add_field('file', body, filename=key.split('/')[-1])
+                async with session.post(wa_upload_url, params=params, data=mp, timeout=120) as upr:
+                    utxt = await upr.text()
+                    if 200 <= upr.status < 300:
+                        uj = await upr.json()
+                        media_id = uj.get("id")
+                        await file_uploads_collection.update_one({"upload_id": upload_id}, {"$set": {"provider_media_id": media_id}})
+                        return media_id
+                    else:
+                        await file_uploads_collection.update_one({"upload_id": upload_id}, {"$set": {"provider_upload_error": utxt}})
+                        return None
+    except Exception as e:
+        await file_uploads_collection.update_one({"upload_id": upload_id}, {"$set": {"provider_upload_error": str(e)}})
+        return None
+
*** End Patch
*** Begin Patch
*** Add File: services/common/cassandra_client.py
+"""
+cassandra_client.py
+Simple wrapper to initialize Cassandra/Scylla session and provide helper functions.
+This module is blocking (cassandra-driver). Use asyncio.to_thread when calling execute
+from async code.
+"""
+import os
+from cassandra.cluster import Cluster
+
+_session = None
+
+def init_cassandra():
+    global _session
+    if _session:
+        return _session
+    contact_points = os.getenv("CASSANDRA_CONTACT_POINTS", "scylla:9042").split(",")
+    keyspace = os.getenv("CASSANDRA_KEYSPACE", "chat4all")
+    cluster = Cluster(contact_points)
+    session = cluster.connect()
+    # ensure keyspace exists (idempotent)
+    session.execute(f"CREATE KEYSPACE IF NOT EXISTS {keyspace} WITH replication = {{'class':'NetworkTopologyStrategy','datacenter1':'3'}};")
+    session.set_keyspace(keyspace)
+    _session = session
+    return _session
+
+def get_session():
+    return _session or init_cassandra()
+
*** End Patch
*** Begin Patch
*** Add File: scripts/migrate_mongo_to_scylla.py
+#!/usr/bin/env python3
+"""
+Simple migrator to backfill messages from MongoDB into Scylla/Cassandra.
+Run on a maintenance machine with network access to both DBs.
+"""
+import os
+from pymongo import MongoClient
+from cassandra.cluster import Cluster
+from datetime import datetime
+import time
+
+MONGO_URI = os.getenv("MONGO_URI", "mongodb://mongo:27017")
+CASS_CONTACT = os.getenv("CASSANDRA_CONTACT_POINTS", "scylla:9042").split(",")
+KEYSPACE = os.getenv("CASSANDRA_KEYSPACE", "chat4all")
+
+def main():
+    mongo = MongoClient(MONGO_URI).chat4all
+    coll = mongo.messages
+    cluster = Cluster(CASS_CONTACT)
+    session = cluster.connect()
+    session.execute(f"CREATE KEYSPACE IF NOT EXISTS {KEYSPACE} WITH replication = {{'class':'NetworkTopologyStrategy','datacenter1':'3'}};")
+    session.set_keyspace(KEYSPACE)
+    # ensure tables exist - idempotent simple DDL
+    session.execute("""
+    CREATE TABLE IF NOT EXISTS messages_by_id (
+      message_id text PRIMARY KEY,
+      conversation_id text,
+      created_at timestamp,
+      sender_id text,
+      recipient_ids list<text>,
+      state text,
+      payload_type text,
+      payload_ref text
+    );
+    """)
+
+    cursor = coll.find().sort("created_at", 1)
+    count = 0
+    for doc in cursor:
+        try:
+            created_at = datetime.utcfromtimestamp(doc.get("created_at", time.time()))
+            session.execute(
+                "INSERT INTO messages_by_id (message_id, conversation_id, created_at, sender_id, recipient_ids, state, payload_type, payload_ref) VALUES (%s,%s,%s,%s,%s,%s,%s,%s)",
+                (doc.get("message_id"), doc.get("conversation_id"), created_at, doc.get("sender_id"), doc.get("recipient_ids", []), doc.get("state"), doc.get("payload_type"), str(doc.get("payload_ref")))
+            )
+            count += 1
+            if count % 1000 == 0:
+                print("migrated", count)
+        except Exception as e:
+            print("error migrating doc", doc.get("message_id"), e)
+    print("done. migrated", count)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: services/api_frontend/app/observability_otel.py
+"""
+OpenTelemetry initialization for api_frontend.
+Call `init_tracing()` early in app startup.
+"""
+import os
+from opentelemetry import trace
+from opentelemetry.sdk.resources import Resource
+from opentelemetry.sdk.trace import TracerProvider
+from opentelemetry.sdk.trace.export import BatchSpanProcessor
+from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
+from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
+from opentelemetry.instrumentation.aiohttp_client import AioHttpClientInstrumentor
+
+def init_tracing(app):
+    otel_endpoint = os.getenv("OTEL_COLLECTOR_ENDPOINT", "http://otel-collector:4317")
+    resource = Resource.create({"service.name":"api_frontend"})
+    provider = TracerProvider(resource=resource)
+    exporter = OTLPSpanExporter(endpoint=otel_endpoint, insecure=True)
+    provider.add_span_processor(BatchSpanProcessor(exporter))
+    trace.set_tracer_provider(provider)
+    FastAPIInstrumentor.instrument_app(app)
+    AioHttpClientInstrumentor().instrument()
+
*** End Patch
*** Begin Patch
*** Add File: services/worker/app/observability_otel.py
+"""
+OpenTelemetry initialization for worker.
+Call `init_tracing()` before starting worker processing.
+"""
+import os
+from opentelemetry import trace
+from opentelemetry.sdk.resources import Resource
+from opentelemetry.sdk.trace import TracerProvider
+from opentelemetry.sdk.trace.export import BatchSpanProcessor
+from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
+from opentelemetry.instrumentation.aiohttp_client import AioHttpClientInstrumentor
+from opentelemetry.instrumentation.aiokafka import AiokafkaInstrumentor
+
+def init_tracing():
+    otel_endpoint = os.getenv("OTEL_COLLECTOR_ENDPOINT", "http://otel-collector:4317")
+    resource = Resource.create({"service.name":"worker"})
+    provider = TracerProvider(resource=resource)
+    exporter = OTLPSpanExporter(endpoint=otel_endpoint, insecure=True)
+    provider.add_span_processor(BatchSpanProcessor(exporter))
+    trace.set_tracer_provider(provider)
+    AioHttpClientInstrumentor().instrument()
+    AiokafkaInstrumentor().instrument()
+
*** End Patch
*** Begin Patch
*** Add File: services/worker/app/circuit_breaker.py
+"""
+Simple circuit-breaker wrapper using aiobreaker for adapter calls.
+Adapters/Worker can use `breaker.call(adapter_fn, *args, **kwargs)`
+"""
+from aiobreaker import CircuitBreaker
+import os
+
+FAIL_MAX = int(os.getenv("CB_FAIL_MAX", "5"))
+RESET_TIMEOUT = int(os.getenv("CB_RESET_TIMEOUT", "60"))
+
+breaker = CircuitBreaker(fail_max=FAIL_MAX, reset_timeout=RESET_TIMEOUT)
+
*** End Patch
*** Begin Patch
*** Modify File: services/worker/app/adapter_client.py
@@
-import asyncio
-import aiohttp
-import json
-from typing import Optional
+import asyncio
+import aiohttp
+import json
+from typing import Optional
+from .circuit_breaker import breaker
@@
     async def send_to_adapter(self, url_base: str, message: dict, recipient_id: str, timeout_override: int = None):
         url = f"{url_base.rstrip('/')}/send"
         payload = {"message": message, "recipient_id": recipient_id}
         timeout = aiohttp.ClientTimeout(total=timeout_override or self.timeout)
-        async with self.session.post(url, json=payload, timeout=timeout) as resp:
-            text = await resp.text()
-            if resp.status >= 200 and resp.status < 300:
-                try:
-                    return await resp.json()
-                except:
-                    return {"status": "ok", "raw": text}
-            else:
-                raise AdapterError(f"Adapter returned {resp.status}: {text}")
+        # Use circuit-breaker to avoid cascading failures to adapters
+        async def _do():
+            async with self.session.post(url, json=payload, timeout=timeout) as resp:
+                text = await resp.text()
+                if resp.status >= 200 and resp.status < 300:
+                    try:
+                        return await resp.json()
+                    except:
+                        return {"status": "ok", "raw": text}
+                else:
+                    raise AdapterError(f"Adapter returned {resp.status}: {text}")
+        # breaker.call will raise CircuitBreakerError if open
+        return await breaker.call(_do)
*** End Patch
*** Begin Patch
*** Modify File: services/api_frontend/requirements.txt
@@
 fastapi
 uvicorn[standard]
 aiokafka
 motor
 aioredis
 pydantic
-boto3
-pyjwt
-botocore
+ boto3
+ pyjwt
+ botocore
+ opentelemetry-api
+ opentelemetry-sdk
+ opentelemetry-instrumentation-fastapi
+ opentelemetry-instrumentation-aiohttp-client
*** End Patch
*** Begin Patch
*** Modify File: services/worker/requirements.txt
@@
 aiokafka
 aiohttp
 motor
 aioredis
+opentelemetry-api
+opentelemetry-sdk
+opentelemetry-instrumentation-aiokafka
+opentelemetry-instrumentation-aiohttp-client
+cassandra-driver
+aiobreaker
*** End Patch
*** Begin Patch
*** Add File: observability/otel/collector-config.yml
+receivers:
+  otlp:
+    protocols:
+      grpc:
+      http:
+exporters:
+  logging:
+    loglevel: debug
+  prometheus:
+    endpoint: "0.0.0.0:8888"
+processors:
+  batch:
+service:
+  pipelines:
+    traces:
+      receivers: [otlp]
+      processors: [batch]
+      exporters: [logging]
+    metrics:
+      receivers: [otlp]
+      processors: [batch]
+      exporters: [prometheus]
+
*** End Patch
*** Begin Patch
*** Add File: helm/charts/scylla/README.md
+Scylla deployment notes:
+- Prefer Scylla Operator or official helm chart for production.
+- Ensure replication factor >=3 and datacenter-aware topology.
+
*** End Patch
*** Begin Patch
*** Add File: helm/charts/redis-cluster/README.md
+Redis Cluster:
+- Deploy 6 nodes (3 masters + 3 replicas).
+- Use Redis Cluster client (redis-py-cluster) in app or redis-py's cluster support.
+
*** End Patch
*** Begin Patch
*** Add File: helm/charts/minio-distributed/README.md
+MinIO distributed notes:
+- Deploy MinIO in distributed mode (4+ nodes).
+- Use Erasure Coding and kms-provider.
+
*** End Patch
*** Begin Patch
*** Add File: helm/charts/redpanda/README.md
+Redpanda notes:
+- Use multiple brokers, configure replication_factor >= 3
+- If using KRaft, configure controller nodes accordingly.
+
*** End Patch
*** Begin Patch
*** Modify File: services/api_frontend/app/routes.py
@@
 from .models import SendMessageReq, SendMessageResp, InitUploadResp, HealthResp
 from .auth import get_current_user, create_jwt
 from .db import messages_collection, file_uploads_collection, redis
 from .kafka_producer import kafka_producer
 from .websocket_mgr import ws_manager
 import boto3
 from botocore.client import Config
+from .webhooks import router as webhooks_router
+from .uploads import router as uploads_router
@@
 router = APIRouter()
+
+# include webhooks and uploads routers
+router.include_router(webhooks_router, prefix="/v1")
+router.include_router(uploads_router, prefix="/v1")
*** End Patch
*** Begin Patch
*** Add File: services/api_frontend/README_OBSERVABILITY.md
+Observability / Tracing
+----------------------
+- The api_frontend has OpenTelemetry bootstrap in services/api_frontend/app/observability_otel.py.
+- Configure OTEL_COLLECTOR_ENDPOINT in env (defaults to otel-collector:4317).
+- Expose /metrics already available in main.py for Prometheus scraping.
+
*** End Patch
*** Begin Patch
*** Add File: services/worker/README_OBSERVABILITY.md
+Observability / Tracing
+----------------------
+- The worker has OpenTelemetry bootstrap in services/worker/app/observability_otel.py.
+- Ensure OTEL_COLLECTOR_ENDPOINT points to collector.
+- Worker exposes /metrics on WORKER_METRICS_PORT (default 8000) via the small metrics server.
+
*** End Patch
*** Begin Patch
*** Add File: docs/run_migration.md
+Run Migration (Mongo -> Scylla)
+------------------------------
+1. Ensure Scylla contact points are reachable and CASSANDRA_KEYSPACE env var set.
+2. Configure MONGO_URI and CASSANDRA_CONTACT_POINTS in env.
+3. Run:
+
+   python3 scripts/migrate_mongo_to_scylla.py
+
+This will backfill messages into `messages_by_id`. Use dual-write approach before cutover.
+
*** End Patch
*** End Patch
